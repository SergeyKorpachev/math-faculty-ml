{"cells":[{"cell_type":"markdown","source":["# Семинар 6: Логистическая регрессия и метод опорных векторов (SVM)."],"metadata":{"id":"p3h8UViJg8oD"},"id":"p3h8UViJg8oD"},{"cell_type":"markdown","id":"nearby-anderson","metadata":{"id":"nearby-anderson"},"source":["## Вступление\n","Сегодня мы познакомимся с логистической регрессией, узнаем её особенности во время обучения и сравним с SVM. Обобщим задачу оптимизации внутри SVM с целью замены ядровой функции. Так же поговорим про метрики классификации и обсудим как их выбирать.\n","\n","### План:\n","1. Логистическая регрессия\n","2. SVM\n","3. Ирисы Фишера. Свойства логистической регрессии и SVM\n","4. Логистическая регрессия и SVM на менее приятных данных\n","5. Метрики классификации\n","6. ROC-кривая\n","7. Бонус: вывод логистической регрессии через правдоподобие"]},{"cell_type":"markdown","id":"russian-closing","metadata":{"id":"russian-closing"},"source":["## Логистическая регрессия\n","\n","Напомним, что мы по-прежнему решаем задачу бинарной классификации, в которой целевая переменная $y$ принимает два значения: -1 и 1. На прошлом семинаре мы обсудили, что эту задачу можно решить при помощи линейного классификатора\n","\n","$$\n","f(x_i, w) = \\mathrm{sign}\\left(\\langle x_i, w \\rangle\\right).\n","$$\n","\n","Функция потерь для такой задачи – это сумма индикаторов того, что предсказание сделано неверно:\n","\n","$$Q(X, w) = \\frac{1}{\\ell}\\sum_{i = 1}^{\\ell}[y_i \\ne \\mathrm{sign}\\left(\\langle x_i, w \\rangle\\right)].$$\n","\n","На лекциях мы обсуждали, что эту идею можно удобно записать через функцию отступа:\n","\n","$$\n","Q(X, w) = \\frac{1}{\\ell}\\sum_{i = 1}^{\\ell}[y_i \\langle x_i, w \\rangle < 0].\n","$$\n","\n","Такую функцию проблематично дифференцировать по $w$, потому что даже в местах, где градиент существует, он равен нулю. Вместо этого будем минимизировать некоторую функцию $\\tilde{Q}(X, w)$, являющуюся верхней оценкой для $Q(X, w)$, и надеяться, что минимизация $\\tilde{Q}(X, w)$ позволит достаточно хорошо минимизировать и $Q(X, w)$.\n","\n","Логистическая регрессия предлагает использовать логистическую функцию потерь:\n","\n","$$\n","\\tilde{Q}(X, w) = \\frac{1}{\\ell}\\sum_{i = 1}^{\\ell}\\log(1 + e^{-y_i \\langle x_i, w \\rangle}) \\rightarrow \\min_w.\n","$$"]},{"cell_type":"code","execution_count":null,"outputs":[],"source":["# !pip install mlxtend --upgrade"],"metadata":{"id":"wF-h2j80gw1V"},"id":"wF-h2j80gw1V"},{"cell_type":"code","execution_count":null,"id":"curious-exemption","metadata":{"id":"curious-exemption"},"outputs":[],"source":["import itertools\n","\n","import matplotlib.gridspec as gridspec\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","from mlxtend.plotting import plot_decision_regions\n","from sklearn.datasets import load_iris\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score, f1_score, roc_curve\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler"]},{"cell_type":"code","execution_count":null,"id":"catholic-tunnel","metadata":{"id":"catholic-tunnel"},"outputs":[],"source":["x = np.concatenate((np.linspace(-3, 0, 500), np.linspace(0, 3, 500)))\n","np.random.seed(123)\n","y = np.ones(1000)\n","w = np.ones(1000)\n","M = y * x * w\n","\n","plt.figure(figsize=(8, 6))\n","plt.plot(x * w, x < 0, label=r\"$\\tilde {Q}(X, w)$, zero-one loss\")\n","plt.plot(M, np.log2(1 + np.exp(-M)), label=\"$Q'(X, w)$, logistic loss\")\n","plt.grid()\n","plt.legend();"]},{"cell_type":"markdown","id":"positive-memorabilia","metadata":{"id":"positive-memorabilia"},"source":["Для получения классической задачи логистической регрессии остаётся сделать один шаг и немного изменить постановку задачи. Предположим, что мы хотим решать задачу **мягкой** классификации, то есть предсказывать не метку класса, а вероятность того, что наблюдение принадлежит к классу. Понятно, что мы всегда можем перейти от мягкой классификации к жёсткой, выбрав порог принадлежности к положительному классу."]},{"cell_type":"markdown","id":"useful-springer","metadata":{"id":"useful-springer"},"source":["**Задание 1.** Поясните, почему для решения задачи мягкой классификации классификатор $f(x_i, w) = \\left(\\langle x_i, w \\rangle\\right)$ – не лучший выбор."]},{"cell_type":"markdown","id":"contained-preparation","metadata":{"id":"contained-preparation"},"source":["**Ответ:**\n","\n","Полученное скалярное произведение необязательно будет лежать в отрезке $[0, 1]$.\n","\n","Мы можем решить названную проблему, подставив $\\left(\\langle x_i, w \\rangle\\right)$ в некоторую функцию, областью значений которой является промежуток от 0 до 1. В логистической регрессии такой функцией выступает **сигмоида**:\n","\n","$$\n","\\sigma(x) = \\dfrac{e^x}{1 + e^x} = \\dfrac{1}{1 + e^{-x}}.\n","$$"]},{"cell_type":"markdown","id":"romantic-yield","metadata":{"id":"romantic-yield"},"source":["**Задание 2.** Сигмоида обаладет замечательнеым свойством, которое значительно упрощает вычисление градиентов при градиентном спуске:\n","\n","$$\n","\\sigma(x)'_x = \\sigma(x)(1 - \\sigma(x)).\n","$$\n","\n","Покажите, что это действительно так.\n","\n","**Решение:**\n","\n","$$\n","\\sigma(x)' = \\frac{e^x(1 + e^x) - e^{2x}}{(1 + e^x)^2} = \\frac{e^x}{1 + e^x}\\frac{1}{1 + e^x} = \\sigma(x)(1-\\sigma(x)).\n","$$"]},{"cell_type":"markdown","id":"reverse-transsexual","metadata":{"id":"reverse-transsexual"},"source":["Путём хитрых математических преобразований можно показать, что при использовании сигмоиды (= при решении задачи мягкой классификации) $\\tilde{Q}(X, w)$ можно записать в следующем виде:\n","\n","$$\n","\\tilde{Q}(X, w) = -\\frac{1}{\\ell} \\sum_{i=1}^{\\ell} [y_i = 1]\\log\\sigma(\\langle x_i, w \\rangle) + [y_i = -1]\\log(1-\\sigma(\\langle x_i, w \\rangle)\n","$$\n","\n","Эта функция называется log-loss или кросс-энтропией между истинной целевой переменной и предсказанными вероятностями.\n","\n","**А почему? А потому:**\n","\n","$$\n","\\tilde{Q}(X, w) = -\\frac{1}{\\ell} \\sum_{i=1}^{\\ell} [y_i = 1]\\log\\sigma(\\langle x_i, w \\rangle) + [y_i = -1]\\log(1-\\sigma(\\langle x_i, w \\rangle) =\\\\\n","$$\n","$$\n","=-\\frac{1}{\\ell} \\sum_{i=1}^{\\ell} [y_i = 1]\\log\\left(\\frac{1}{1 + e^{-\\langle x_i, w\\rangle}}\\right) + [y_i = -1](\\log\\left(1-\\frac{1}{1 + e^{-\\langle x_i, w\\rangle}}\\right)) =\\\\\n","$$\n","$$\n","=-\\frac{1}{\\ell} \\sum_{i=1}^{\\ell} [y_i = 1]\\log\\left(\\frac{1}{1 + e^{-\\langle x_i, w\\rangle}}\\right) + [y_i = -1](\\log\\left(\\frac{1}{1 + e^{\\langle x_i, w\\rangle}}\\right)) =\\\\\n","$$\n","$$\n","=\\frac{1}{\\ell} \\sum_{i=1}^{\\ell} [y_i = 1]\\log(1 + e^{-\\langle x_i, w \\rangle}) + [y_i = -1]\\log(1 + e^{\\langle x_i, w \\rangle}) =\\\\\n","$$\n","$$\n","=\\frac{1}{\\ell} \\sum_{i=1}^{\\ell}\\log(1 + e^{-y_i\\langle x_i, w\\rangle}) = \\tilde{Q}(X, w)\n","$$"]},{"cell_type":"markdown","id":"anticipated-amplifier","metadata":{"id":"anticipated-amplifier"},"source":["Регуляризация вводится таким же образом, как это было в случае линейной регрессии. Например, функция потерь для $\\ell$-$2$ регуляризации выглядит так:\n","\n","$$\n","\\tilde{Q}_{reg}(X, w) = \\tilde{Q}(X, w) + \\frac{1}{2}\\lambda\\|w\\|^2_2.\n","$$"]},{"cell_type":"markdown","id":"lovely-empire","metadata":{"id":"lovely-empire"},"source":["**Саммари по логистической регрессии:**\n","\n","- Данные: $y_i \\in \\{-1, 1\\}$, $X$\n","\n","- Предсказания:\n","\n","    - По умолчанию предсказывает вероятности:\n","  $$\n","  \\hat{p}_i = \\sigma(\\langle x_i, w\\rangle) = \\frac{1}{1 + e^{-\\langle x_i, w\\rangle}}\n","  $$\n","    \n","    - Если хотим перейти к метке, то сравниваем вероятность с порогом $t$:\n","    \n","  $$\n","  \\hat{y}_i = \\begin{cases}\n","  1,&\\text{ если } \\hat{p}_i > t,\\\\\n","  -1,&\\text{ иначе.}\n","  \\end{cases}\n","  $$\n","    \n","- Обучение:\n","\n","$$\n","\\tilde{Q}(X, w) = -\\frac{1}{\\ell} \\sum_{i=1}^{\\ell} [y_i = 1]\\log\\sigma(\\langle x_i, w \\rangle) + [y_i = -1]\\log(1-\\sigma(\\langle x_i, w \\rangle) \\to \\min_w\n","$$"]},{"cell_type":"markdown","id":"separated-superintendent","metadata":{"id":"separated-superintendent"},"source":["## Метод опорных векторов (SVM)\n","\n","Метод опорных векторов — математически строго обоснованный метод, идея которого состоит в максимизации ширины разделяющей полосы между классами. Так как для подробного вывода SVM требуется уверенное владение методами квадратичной оптимизации, мы разберём только идею и практическую реализацию метода.\n","\n","Мы по-прежнему решаем задачу бинарной классификации и используем классификатор $f(x_i, w) = \\mathrm{sign}(\\langle x_i, w\\rangle)$. Предположим, что мы работаем с линейно разделимой выборкой. Определим отступ как минимальное расстояние от точек выборки до разделяющей поверхности классификатора:\n","$$\n","\\rho(x_i, \\langle x, w\\rangle) =  \\min_i\\dfrac{|{\\langle x_i, w\\rangle|}}{\\|w\\|}\n","$$\n","\n","Задача состоит в том, чтобы максимизировать этот отступ:\n","\n","$$\n","\\rho(x_i, \\langle x, w\\rangle) =  \\min_i\\dfrac{|{\\langle x_i, w\\rangle|}}{\\|w\\|} \\to \\max_w.\n","$$\n","\n","Воспользуемся картинкой из Википедии, чтобы лучше понять эту идею:\n","\n","[<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/7/72/SVM_margin.png/1920px-SVM_margin.png\" alt=\"drawing\" width=\"400\"/>](https://en.wikipedia.org/wiki/Support-vector_machine#/media/File:SVM_margin.png)\n","\n","Заметим, что при делении весов на положительное число ответы классификатора не меняются (веса просто пропорционально прошкалируются). Поделим все веса на\n","\n","$$\\min_i |\\langle x_i, w\\rangle| > 0.$$\n","\n","Тогда будет верно, что $\\min_i |\\langle x_i, w\\rangle| = 1$, а значит отступ можно переписать как\n","\n","$$\n","\\rho(\\langle x_i, w\\rangle) = \\frac{1}{\\|w\\|}.\n","$$\n","\n","Получаем задачу SVM в линейно-разделимом случае:\n","\n","$$\n","\\begin{cases}\n","\\|w\\|^2 \\to \\min_{w}, \\\\\n","y_i(\\langle x_i, w\\rangle) \\ge 1, \\\\\n","\\end{cases}\n","$$\n","\n","Если выборка не является линейно-разделимой, то нам придётся позволить линейному классификатору допускать ошибки на некоторых наблюдениях. Тогда задача превращается в поиск оптимального выбора между максимизацией ширины разделяющей полосы и ошибок классификации:\n","\n","$$\n","\\begin{cases}\n","\\|w\\|^2 + C \\sum_{i = 1}^{\\ell} \\xi_i \\to \\min_{w, \\xi_i}, \\\\\n","y_i(\\langle x_i, w\\rangle) \\ge 1 - \\xi_i, \\\\\n","\\xi_i \\ge 0\n","\\end{cases}\n","$$\n","\n","$C$ – параметр, который позволяет регулировать пропорции этого выбора. Чем больше $C$, тем больше штраф за неверную классификацию.\n","\n","<img src=\"https://scikit-learn.org/stable/_images/sphx_glr_plot_linearsvc_support_vectors_001.png\" alt=\"drawing\" width=\"800\"/>\n","\n","Путём хитрых математических преобразований (объединив ограничения) можно показать, что итоговая функция потерь SVM выглядит следующим образом:\n","\n","$$\n","Q(X, w) = C\\sum_{i=1}^{l} \\max\\{0, 1 - y_i(\\langle x_i, w\\rangle)\\} + \\|w\\|^2 \\to \\min_w\n","$$"]},{"cell_type":"markdown","id":"technological-cooperation","metadata":{"id":"technological-cooperation"},"source":["### Ирисы Фишера. Свойства логистической регрессии и SVM"]},{"cell_type":"markdown","id":"anonymous-reporter","metadata":{"id":"anonymous-reporter"},"source":["Рассмотрим свойства логистической регрессии и метода опорных векторов на примере классического набора данных [\"Ирисы Фишера\"](https://ru.wikipedia.org/wiki/Ирисы_Фишера). Этот набор состоит из 150 наблюдений, каждое из которых представляет собой четыре измерения: длина наружной доли околоцветника (`sepal length`), ширина наружной доли околоцветника (`sepal width`), длина внутренней доли околоцветника (`petal length`), ширина внутренней доли околоцветника (`petal width`). Каждое наблюдение относится к одному из трёх классов ириса: `setosa`, `versicolor` или `virginica`. Задача состоит в том, чтобы по измерениям предсказать класс цветка.\n","\n","<img src=\"https://www.embedded-robotics.com/wp-content/uploads/2022/01/Iris-Dataset-Classification.png\" alt=\"drawing\" width=\"800\"/>"]},{"cell_type":"code","execution_count":null,"id":"medieval-cowboy","metadata":{"id":"medieval-cowboy"},"outputs":[],"source":["data = load_iris()\n","X = pd.DataFrame(data[\"data\"], columns=data[\"feature_names\"])\n","y = data[\"target\"]\n","X.head()"]},{"cell_type":"markdown","id":"conservative-target","metadata":{"id":"conservative-target"},"source":["**Задание 1.** Перейдём к задаче бинарной классификации: будем предсказывать принадлежность цветка к виду `versicolor` против принадлежности ко всем прочим видам. Перекодируйте зависимую переменную так, чтобы цветки вида `versicolor` имели метку 1, а прочих видов – метку -1."]},{"cell_type":"code","execution_count":null,"id":"signal-stereo","metadata":{"id":"signal-stereo"},"outputs":[],"source":["# Перекодировка\n","# Как это можно сделать?\n","# Ваш код сюда :)"]},{"cell_type":"markdown","id":"governmental-repository","metadata":{"id":"governmental-repository"},"source":["**Задание 2**. Будем работать с двумя признаками: `sepal length (cm)` и `sepal width (cm)`. Отделите их в отдельную матрицу. Разделите выборку на обучающую и тестовую, долю тестовой выборки укажите равной 0.3. Отмасштабируйте выборки при помощи StandardScaler. Постройте диаграмму рассеяния по тренировочной выборке и убедитесь, что данные линейно не разделимы."]},{"cell_type":"code","execution_count":null,"id":"ordered-pavilion","metadata":{"id":"ordered-pavilion"},"outputs":[],"source":["np.random.seed(123)\n","\n","# Ваш код сюда :)\n","\n","\n","# Отмасштабируйте признаки\n"]},{"cell_type":"markdown","id":"divine-lease","metadata":{"id":"divine-lease"},"source":["**Задание 3.** Обучите логистическую регрессию и **SVM с линейным ядром** на тренировочной выборке и убедитесь, что полученные оценки весов действительно различаются. Убедитесь, что `accuracy`, возможно, не подходит в качестве метрики для данной задачи и рассчитайте `f1-меру` на тестовой выборке. Какой алгорим показал более высокое качество?"]},{"cell_type":"code","execution_count":null,"id":"interim-rabbit","metadata":{"id":"interim-rabbit"},"outputs":[],"source":["from sklearn.metrics import classification_report\n","from sklearn.svm import SVC"]},{"cell_type":"code","execution_count":null,"id":"spiritual-definition","metadata":{"id":"spiritual-definition"},"outputs":[],"source":["# Ваш код сюда :)\n","\n","\n","\n","# Обучите логистическую регрессию и SVM\n","\n","\n","print(classification_report(y_test, lr_pred))\n","print(classification_report(y_test, svm_pred))"]},{"cell_type":"markdown","id":"manufactured-breeding","metadata":{"id":"manufactured-breeding"},"source":["Теперь посмотрим, как различаются решающие поверхности алгоритмов. Код ниже построит решающие поверхности для классификаторов. Чтобы он заработал, нужно обязательно сделать `fit` для `lr` и `svm` выше."]},{"cell_type":"code","execution_count":null,"id":"27284d50","metadata":{"id":"27284d50"},"outputs":[],"source":["gs = gridspec.GridSpec(1, 2)\n","\n","fig = plt.figure(figsize=(10, 8))\n","\n","labels = [\"Logistic Regression\", \"SVM\"]\n","for clf, lab, grd in zip([lr, svm], labels, itertools.product([0, 1], repeat=2)):\n","    ax = plt.subplot(gs[grd[0], grd[1]])\n","    fig = plot_decision_regions(\n","        X=np.array(X_train), y=np.array(y_train), clf=clf, legend=2\n","    )\n","    plt.title(lab)\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"impressive-chamber","metadata":{"id":"impressive-chamber"},"outputs":[],"source":["gs = gridspec.GridSpec(1, 2)\n","\n","fig = plt.figure(figsize=(10, 8))\n","\n","labels = [\"Logistic Regression\", \"SVM\"]\n","for clf, lab, grd in zip([lr, svm], labels, itertools.product([0, 1], repeat=2)):\n","    ax = plt.subplot(gs[grd[0], grd[1]])\n","    fig = plot_decision_regions(\n","        X=np.array(X_train), y=np.array(y_train), clf=clf, legend=2\n","    )\n","    plt.title(lab)\n","\n","plt.show()"]},{"cell_type":"markdown","id":"satellite-vacation","metadata":{"id":"satellite-vacation"},"source":["Теперь изучим свойства каждого классификатора по-отдельности. Начнём с логистической регрессии.\n","\n","**Задание 3.** Обучите три различные логистические регрессии с разным параметром регуляризации $\\alpha$ (обратите внимание, что в реализации `sklearn` $C = 1/\\alpha$). Как изменяется разделяющая поверхность в зависимости от $\\alpha$?"]},{"cell_type":"code","execution_count":null,"id":"proper-accused","metadata":{"id":"proper-accused"},"outputs":[],"source":["lr1 = LogisticRegression(C=0.01)\n","lr2 = LogisticRegression(C=0.05)\n","lr3 = LogisticRegression(C=10)"]},{"cell_type":"code","execution_count":null,"id":"thorough-architect","metadata":{"id":"thorough-architect"},"outputs":[],"source":["gs = gridspec.GridSpec(1, 3)\n","\n","fig = plt.figure(figsize=(15, 8))\n","\n","labels = [\"C = 0.01\", \"C = 0.05\", \"C = 10\"]\n","for clf, lab, grd in zip(\n","    [lr1, lr2, lr3], labels, itertools.product([0, 1, 2], repeat=2)\n","):\n","    clf.fit(X_train, y_train)\n","    ax = plt.subplot(gs[grd[0], grd[1]])\n","    fig = plot_decision_regions(X=X_train, y=np.array(y_train), clf=clf, legend=2)\n","    plt.title(lab)\n","\n","plt.show()"]},{"cell_type":"markdown","id":"behind-northern","metadata":{"id":"behind-northern"},"source":["Перейдём к SVM.\n","\n","**Задание 4.** Обучите три SVM с линейным ядром с разным параметром регуляризации $C$. Как изменяется разделяющая поверхность в зависимости от $C$?"]},{"cell_type":"code","execution_count":null,"id":"found-shadow","metadata":{"id":"found-shadow"},"outputs":[],"source":["svc1 = SVC(C=0.04, kernel=\"linear\")\n","svc2 = SVC(C=1, kernel=\"linear\")\n","svc3 = SVC(C=10, kernel=\"linear\")"]},{"cell_type":"code","execution_count":null,"id":"imperial-primary","metadata":{"id":"imperial-primary"},"outputs":[],"source":["gs = gridspec.GridSpec(1, 3)\n","\n","fig = plt.figure(figsize=(15, 8))\n","\n","labels = [\"C = 0.04\", \"C = 1\", \"C = 10\"]\n","for clf, lab, grd in zip(\n","    [svc1, svc2, svc3], labels, itertools.product([0, 1, 2], repeat=2)\n","):\n","    clf.fit(X_train, y_train)\n","    ax = plt.subplot(gs[grd[0], grd[1]])\n","    fig = plot_decision_regions(X=X_train, y=np.array(y_train), clf=clf, legend=2)\n","    plt.title(lab)\n","\n","plt.show()"]},{"cell_type":"markdown","id":"touched-resource","metadata":{"id":"touched-resource"},"source":["### Логистическая регрессия и SVM на менее приятных данных (если есть время)"]},{"cell_type":"markdown","id":"accessible-cleanup","metadata":{"id":"accessible-cleanup"},"source":["Мы будем работать с [набором данных](https://www.kaggle.com/piyushgoyal443/red-wine-dataset?select=wineQualityReds.csv), содержащим информацию о характеристиках вина. Каждое наблюдение принадлежит к одному из 10 категорий качества вина, и наша задача заключается в том, что предсказать эту категорию."]},{"cell_type":"code","source":["import kagglehub\n","\n","# Загрузим последнюю версию\n","path = kagglehub.dataset_download(\"piyushgoyal443/red-wine-dataset\")\n","\n","print(\"Path to dataset files:\", path)"],"metadata":{"id":"l1DykVkYon-u"},"id":"l1DykVkYon-u","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Либо так:\n","\n","#!wget https://raw.githubusercontent.com/SergeyKorpachev/math-faculty-ml/refs/heads/main/2026/seminars/seminar06/wineQualityReds.csv"],"metadata":{"id":"xXP12hGboDsT"},"id":"xXP12hGboDsT","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"multiple-humor","metadata":{"id":"multiple-humor"},"outputs":[],"source":["data = pd.read_csv(path + \"/\" + \"wineQualityReds.csv\", index_col=0)\n","data.head()"]},{"cell_type":"markdown","id":"german-artwork","metadata":{"id":"german-artwork"},"source":["Как указано в описании набора, в нём нет пропущенных значений, и все переменные являются непрерывными. Целевая переменная – `quality`."]},{"cell_type":"markdown","id":"crazy-moses","metadata":{"id":"crazy-moses"},"source":["**Задание 1.** Перейдём к задаче бинарной классификации и будем предсказывать только наиболее популярную категорию качества. Закодируйте столбец `quality` так, чтобы наиболее частая категория (это категория 5) получила метку 1, а все прочие категории – метку -1."]},{"cell_type":"code","execution_count":null,"id":"prepared-laser","metadata":{"id":"prepared-laser"},"outputs":[],"source":["# Ваш код сюда :)"]},{"cell_type":"markdown","id":"steady-ecology","metadata":{"id":"steady-ecology"},"source":["**Задание 2.** Разделите признаки и целевую переменную. Разделите выборку на тренировочную и тестовую, долю тестовой выборки укажите равной 0.3. При помощи `StandardScaler` отмасштабируйте тренировочную и тестовую выборки."]},{"cell_type":"code","execution_count":null,"id":"beneficial-perry","metadata":{"id":"beneficial-perry"},"outputs":[],"source":["# Ваш код сюда :)\n","\n","\n","\n","# Отмасштабируйте признаки\n"]},{"cell_type":"markdown","id":"quick-replica","metadata":{"id":"quick-replica"},"source":["**Задание 3.** При помощи кросс-валидации (параметры выберите сами) подберите оптимальные значения коэффициентов регуляризации для логистической регрессии и SVM с линейным ядром. Обучите модели с этими параметрами. Убедитесь, что доля правильных ответов – не лучший вариант для нашей задачи и рассчитайте F-меру на тестовой выборке. Какой алгоритм показал себя лучше?\n","\n"," **Бонус для самых отважных:** качество работы SVM можно улучшить за счёт применения ядер, после чего разделяющая поверхность становится нелинейной. Если вам интересно, попросите семинариста рассказать об этом подробнее.  "]},{"cell_type":"code","execution_count":null,"id":"destroyed-cooling","metadata":{"id":"destroyed-cooling"},"outputs":[],"source":["scores_lr = []\n","scores_svm = []\n","\n","# Ваш код сюда :)\n","\n"]},{"cell_type":"code","execution_count":null,"id":"1ae8371e","metadata":{"id":"1ae8371e"},"outputs":[],"source":["scores_lr"]},{"cell_type":"code","execution_count":null,"id":"1372d7ed","metadata":{"id":"1372d7ed"},"outputs":[],"source":["scores_svm"]},{"cell_type":"markdown","id":"c0d90ec9","metadata":{"id":"c0d90ec9"},"source":["## Метрики классификации"]},{"cell_type":"markdown","id":"e1af0fa6","metadata":{"id":"e1af0fa6"},"source":["1.accuracy\n","\n","2.precision\n","\n","3.recall\n","\n","4.f1\n","\n","5.pr-recall curve\n","\n","6.roc - auc"]},{"cell_type":"code","execution_count":null,"id":"e4f13f7e","metadata":{"id":"e4f13f7e"},"outputs":[],"source":["n = 500\n","\n","# True labels\n","y = np.random.randint(0, 2, 500)\n","# Predicted labels\n","p = (np.random.random(500) > 0.5).astype(int)"]},{"cell_type":"code","execution_count":null,"outputs":[],"source":["from sklearn.metrics import confusion_matrix"],"metadata":{"id":"oVM9gxkcgw2N"},"id":"oVM9gxkcgw2N"},{"cell_type":"code","execution_count":null,"id":"58363cb2","metadata":{"id":"58363cb2"},"outputs":[],"source":["def accuracy(y, p):\n","    return sum(y == p) / len(y)\n","\n","def precision(y, p):\n","    (TN, FP), (FN, TP) = confusion_matrix(y, p)\n","\n","    return TP / (TP + FP)\n","\n","def recall(y, p):\n","    (TN, FP), (FN, TP) = confusion_matrix(y, p)\n","\n","    return TP / (TP + FN)\n","\n","def f1(y, p, beta=1):\n","    re = recall(y, p)\n","    pr = precision(y, p)\n","\n","    return (1 + beta**2) * pr * re / (beta**2 * pr + re)"]},{"cell_type":"code","execution_count":null,"id":"c8f45ec2","metadata":{"id":"c8f45ec2"},"outputs":[],"source":["metric_dict = {\"acc\": accuracy, \"pr\": precision, \"re\": recall, \"f1\": f1}\n","for title, func in metric_dict.items():\n","    print(title, func(y, p))"]},{"cell_type":"code","execution_count":null,"id":"c257b86e","metadata":{"id":"c257b86e"},"outputs":[],"source":["print(classification_report(y, p))"]},{"cell_type":"markdown","id":"neutral-notebook","metadata":{"id":"neutral-notebook"},"source":["### ROC-кривая"]},{"cell_type":"markdown","id":"marine-parish","metadata":{"id":"marine-parish"},"source":["Ранее мы узнали, что помимо accuracy в задачах классификации так же используются precision, recall и f-мера. Теперь пришло время познакомиться с ещё одной метрикой – ROC AUC.\n","\n","Для начала вспомним, что мы работаем с матрицей ошибок:\n","\n","|       | alg = 1          | alg = -1    |\n","|-------| -----------------|-------------|\n","|y = 1  |TP                |FN           |\n","|y = -1 |FP                | TN          |\n","\n","Определим следующие величины:\n","\n","$$\n","TPR \\text{ (true positive rate, recall, sensitivity)} = \\dfrac{TP}{TP + FN}\n","$$\n","– доля правильно предсказанных объектов положительного класса.\n","\n","$$\n","FPR \\text{ (false positive rate, 1 - specificity)} = \\dfrac{FP}{FP + TN}\n","$$\n","– доля неправильно предсказанных объектов отрицательного класса.\n","\n","Рассмотрим задачу мягкой классификации: мы предказываем вероятности принадлежности наблюдения к положительному и отрицательному классам. Тогда TPR и FPR будут зависеть от порога для вероятности, выше которого наблюдение будет отнесено к положительному классу. ROC-кривая строится в координатах $(FPR, TPR)$ и показывает комбинации TPR и FPR при всевозможных значениях порога.\n","\n","Для хорошего классификатора эта кривая является вогнутой, а для идеального классификатора она будет проходить через точку $(0, 1)$ (почему?).\n","\n","[<img src=\"https://upload.wikimedia.org/wikipedia/commons/6/6b/Roccurves.png\" alt=\"drawing\" width=\"350\"/>](https://ru.wikipedia.org/wiki/ROC-кривая)\n","\n","\n","\n","**Задание 1.** Постройте ROC-кривую для следующей выборки."]},{"cell_type":"code","execution_count":null,"id":"convertible-leader","metadata":{"id":"convertible-leader"},"outputs":[],"source":["# True labels\n","y = [-1, 1, 1, -1, 1, 1]\n","# Predicted labels\n","p = [0.5, 0.1, 0.2, 0.9, 0.7, 0.1]"]},{"cell_type":"markdown","id":"worst-machinery","metadata":{"id":"worst-machinery"},"source":["**Решение:**\n","1. Упорядочим наблюдения по **убыванию** ответов алгоритма."]},{"cell_type":"code","execution_count":null,"id":"desirable-bleeding","metadata":{"id":"desirable-bleeding"},"outputs":[],"source":["y = [-1, 1, -1, 1, 1, 1]\n","p = [0.9, 0.7, 0.5, 0.2, 0.1, 0.1]"]},{"cell_type":"markdown","id":"virtual-crack","metadata":{"id":"virtual-crack"},"source":["2. Разобьём единичный квадрат на $(m, n)$ частей, где $m$ – число 1 в $y$, $n$ – число нулей. Стартуем из точки $(0, 0)$. Если значение $y$ равно 1, делаем шаг вверх, а если -1 – вправо. Понятно, что конечная точка нашего маршрута – точка $(1, 1)$.\n","\n","**Важный момент:** если у нескольких объектов значения предсказаний равны, а $y$ – различны, то мы должны сделать ход \"по диагонали\".\n","\n","Если построить кривую по этому алгоритму, то получим:"]},{"cell_type":"code","execution_count":null,"id":"substantial-louis","metadata":{"id":"substantial-louis"},"outputs":[],"source":["fpr, tpr, _ = roc_curve(y, p)\n","\n","plt.figure(figsize=(8, 8))\n","plt.plot(fpr, tpr, label=\"ROC\")\n","plt.axvline(0.5, linestyle=\"dotted\", c=\"red\")\n","plt.axhline(0.25, linestyle=\"dotted\", c=\"red\")\n","plt.axhline(0.5, linestyle=\"dotted\", c=\"red\")\n","plt.axhline(0.75, linestyle=\"dotted\", c=\"red\")\n","plt.axhline(1.0, linestyle=\"dotted\", c=\"red\")\n","plt.title(\"Красные линии показывают разбиение единичного квадрата на m и n частей\")\n","plt.legend();"]},{"cell_type":"markdown","id":"manufactured-lyric","metadata":{"id":"manufactured-lyric"},"source":["3. Полученная кривая и является ROC-кривой.\n","\n","**(Почему этот алгоритм имеет смысл?)**"]},{"cell_type":"markdown","id":"arctic-scott","metadata":{"id":"arctic-scott"},"source":["**Пример с диагональным шагом.**"]},{"cell_type":"code","execution_count":null,"id":"wireless-worship","metadata":{"id":"wireless-worship"},"outputs":[],"source":["p = [0.5, 0.1, 0.2, 0.6, 0.2, 0.3, 0.0]\n","y = [-1, -1, -1, 1, 1, 1, -1]"]},{"cell_type":"code","execution_count":null,"id":"ef94098e","metadata":{"id":"ef94098e"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"bizarre-farming","metadata":{"id":"bizarre-farming"},"outputs":[],"source":["fpr, tpr, _ = roc_curve(y, p)"]},{"cell_type":"code","execution_count":null,"id":"likely-facial","metadata":{"id":"likely-facial"},"outputs":[],"source":["plt.figure(figsize=(8, 8))\n","plt.plot(fpr, tpr, label=\"ROC\")\n","plt.axvline(0.25, linestyle=\"dotted\", c=\"red\")\n","plt.axvline(0.5, linestyle=\"dotted\", c=\"red\")\n","plt.axvline(0.75, linestyle=\"dotted\", c=\"red\")\n","plt.axvline(1.0, linestyle=\"dotted\", c=\"red\")\n","plt.axhline(0.33, linestyle=\"dotted\", c=\"red\")\n","plt.axhline(0.66, linestyle=\"dotted\", c=\"red\")\n","plt.axhline(1.0, linestyle=\"dotted\", c=\"red\")\n","plt.title(\"Красные линии показывают разбиение единичного квадрата на m и n частей\")\n","plt.legend();"]},{"cell_type":"markdown","id":"present-hurricane","metadata":{"id":"present-hurricane"},"source":["**Задание 2.** ROC AUC – площадь под ROC-кривой – равна доле пар наблюдений $(y = 1, y = -1)$, которые алгоритм верно упорядочил. Таким образом, чем больше ROC AUC, тем качественнее отработал классификатор. Вычислите ROC AUC для построенной ROC-кривой из первого примера."]},{"cell_type":"markdown","id":"prescription-indiana","metadata":{"id":"prescription-indiana"},"source":["**Решение:**"]},{"cell_type":"code","execution_count":null,"id":"complicated-istanbul","metadata":{"id":"complicated-istanbul"},"outputs":[],"source":["0.25 * 0.5"]},{"cell_type":"markdown","id":"colored-hebrew","metadata":{"id":"colored-hebrew"},"source":["**Задание 3.** Как выглядит ROC-кривая для случайного классификатора?\n","\n","\n","**Задание 4.** Как по ROC-кривой выбрать порог для бинаризации?"]},{"cell_type":"markdown","id":"expensive-spyware","metadata":{"id":"expensive-spyware"},"source":["В `sklearn` реализовано вычисление значений ROC-кривой и площади под ней."]},{"cell_type":"code","execution_count":null,"id":"corresponding-target","metadata":{"id":"corresponding-target"},"outputs":[],"source":["from sklearn.metrics import roc_curve"]},{"cell_type":"markdown","id":"premier-merchandise","metadata":{"id":"premier-merchandise"},"source":["**Важно:** в `roc_curve` передаются предсказанные вероятности!"]},{"cell_type":"markdown","id":"searching-crystal","metadata":{"id":"searching-crystal"},"source":["**Задание 5.** Постройте ROC-кривую и рассчитайте площадь под ней для логистической регрессии на данных о вине. О чём говорит такая форма кривой?"]},{"cell_type":"code","execution_count":null,"id":"driving-doubt","metadata":{"id":"driving-doubt"},"outputs":[],"source":["lr = LogisticRegression()\n","lr.fit(X_train, y_train)\n","\n","pred = lr.predict_proba(X_test)[:, 1]"]},{"cell_type":"code","execution_count":null,"id":"26a97755","metadata":{"id":"26a97755"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"a95cb0c1","metadata":{"id":"a95cb0c1"},"outputs":[],"source":["x, y, val = roc_curve(y_test, pred)"]},{"cell_type":"code","execution_count":null,"id":"5cae03d8","metadata":{"id":"5cae03d8"},"outputs":[],"source":["plt.scatter(x, y)"]},{"cell_type":"markdown","id":"narrow-martial","metadata":{"id":"narrow-martial"},"source":["### Бонус для самых смелых: вывод логистической регрессии через правдоподобие\n","\n","Перекодируем $y$ так, что её возможные значения – это 0 и 1. Теперь $y$ является случайной величиной из распределения Бернулли. Тогда сигмоида задаёт условную вероятность принадлежности $y$ к положительному классу:\n","\n","$$\n","P(y_i = 1 | x_i, w) = \\sigma(x_i, w).\n","$$\n","\n","Будем искать оценку коэффициентов $w$ при помощи метода максимального правдоподобия.\n","\n","$$\n","\\mathcal{L} = \\sigma(x_i, w)^{\\sum_i y_i}(1 - \\sigma(x_i, w))^{\\sum_i 1 - y_i}\n","$$\n","\n","Логарифмируем:\n","\n","$$\n","\\mathcal{l} = \\sum_i y_i \\log\\sigma(x_i, w) + (1 - y_i)(1 - \\log\\sigma(x_i, w))\n","$$\n","\n","Если теперь взять среднее, то мы получим log-loss, взятый со знаком минус. Таким образом, минимизация функции потерь в логистической регрессии эквивалентна  максимизации правдоподобия в задаче нахождения оценок $w$!"]},{"cell_type":"code","source":[],"metadata":{"id":"m0mP3kHZq9qD"},"id":"m0mP3kHZq9qD","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}