{"cells":[{"cell_type":"markdown","metadata":{"id":"LVufRUQR_euO"},"source":["# Семинар 7: Работа с текстовыми данными."]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2019-09-28T19:10:18.309671Z","start_time":"2019-09-28T19:10:14.654448Z"},"cell_style":"center","id":"gKnshhWe_euP"},"outputs":[],"source":["%pylab inline\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from tqdm.notebook import tqdm\n","from sklearn.datasets import fetch_20newsgroups\n","\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.linear_model import Ridge, LogisticRegression\n","from sklearn.metrics import mean_squared_error, accuracy_score\n","\n","import warnings\n","warnings.simplefilter(\"ignore\")"]},{"cell_type":"markdown","metadata":{"id":"HEQnJxw9_euQ"},"source":["Как правило, модели машинного обучения действуют в предположении, что матрица \"объект-признак\" является вещественнозначной, поэтому при работе с текстами сперва для каждого из них необходимо составить его признаковое описание. Для этого широко используются техники векторизации, tf-idf и пр. Рассмотрим их на примере датасета новостей о разных топиках.\n","\n","Сперва загрузим данные:"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2019-09-28T19:10:18.959288Z","start_time":"2019-09-28T19:10:18.346983Z"},"id":"ZgFLvJAd_euQ"},"outputs":[],"source":["data = fetch_20newsgroups(subset='all', categories=['comp.graphics', 'sci.med'])"]},{"cell_type":"markdown","metadata":{"id":"46JAf-TV_euQ"},"source":["Данные содержат тексты новостей, которые надо классифицировать на два раздела: компьютерные науки и медицинские."]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2019-09-28T19:10:18.987312Z","start_time":"2019-09-28T19:10:18.964475Z"},"id":"qu6en380_euQ"},"outputs":[],"source":["data['target_names']"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2019-09-28T19:10:19.012942Z","start_time":"2019-09-28T19:10:18.997929Z"},"id":"yIR3RtAg_euQ"},"outputs":[],"source":["texts = data['data']\n","target = data['target']"]},{"cell_type":"markdown","metadata":{"id":"ePCvaMmR_euQ"},"source":["Например:"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2019-09-28T19:10:19.060074Z","start_time":"2019-09-28T19:10:19.019838Z"},"scrolled":true,"id":"ppQChgce_euQ"},"outputs":[],"source":["texts[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2019-09-28T19:10:19.164963Z","start_time":"2019-09-28T19:10:19.131795Z"},"id":"MICnKtGZ_euQ"},"outputs":[],"source":["data['target_names'][target[0]]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ETsZaSYh_euQ"},"outputs":[],"source":["texts_train, texts_test, y_train, y_test = train_test_split(\n","    texts, target, test_size=0.2, random_state=10\n",")"]},{"cell_type":"markdown","metadata":{"id":"DZGEgQHb_euQ"},"source":["### Bag-of-words\n","\n","Самый очевидный способ формирования признакового описания текстов — векторизация. Пусть у нас имеется коллекция текстов $D = \\{d_i\\}_{i=1}^l$ и словарь всех слов, встречающихся в выборке $V = \\{v_j\\}_{j=1}^d.$ В этом случае некоторый текст $d_i$ описывается вектором $(x_{ij})_{j=1}^d,$ где\n","$$x_{ij} = \\sum_{v \\in d_i} [v = v_j].$$\n","\n","Таким образом, текст $d_i$ описывается вектором количества вхождений каждого слова из словаря в данный текст."]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2019-09-28T19:10:19.833837Z","start_time":"2019-09-28T19:10:19.175011Z"},"id":"EoicjNVI_euQ"},"outputs":[],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","vectorizer = CountVectorizer(encoding='utf8')\n","_ = vectorizer.fit(texts_train)\n","len(vectorizer.vocabulary_)"]},{"cell_type":"markdown","metadata":{"id":"urgsLngp_euR"},"source":["Результатом является разреженная матрица."]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2019-09-28T19:10:19.851338Z","start_time":"2019-09-28T19:10:19.837509Z"},"id":"ad1Sb97U_euR"},"outputs":[],"source":["vectorizer.transform(texts_train[:1])"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2019-09-28T19:10:19.872060Z","start_time":"2019-09-28T19:10:19.854688Z"},"id":"bgySvheK_euR"},"outputs":[],"source":["print(vectorizer.transform(texts_train[:1]).indices)\n","print(vectorizer.transform(texts_train[:1]).data)"]},{"cell_type":"markdown","metadata":{"id":"uun2FcH2_euR"},"source":["Подберем оптимальные гиперпараметры по сетке и обучим модель. Учить будем логистическую регрессию, так как мы решаем задачу бинарной классификации, а для оценки качества будем использовать точность\n","$$\n","Accuracy(\\hat{y}, y) = \\frac{1}{N} \\sum_{i=1}^N [\\hat{y_i} = y_i]\n","$$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XSQ5LQV5_euR"},"outputs":[],"source":["def train_model(X_train, y_train):\n","    alphas = np.logspace(-1, 3, 10)\n","    searcher = GridSearchCV(LogisticRegression(), [{'C': alphas, 'max_iter': [500]}],\n","                            scoring='accuracy', cv=5, n_jobs=-1)\n","    searcher.fit(X_train, y_train)\n","\n","    best_alpha = searcher.best_params_[\"C\"]\n","    print(\"Best alpha = %.4f\" % best_alpha)\n","\n","    model = LogisticRegression(C=best_alpha, max_iter=500)\n","    model.fit(X_train, y_train)\n","\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fb2lT8kt_euR"},"outputs":[],"source":["X_train = vectorizer.transform(texts_train)\n","X_test = vectorizer.transform(texts_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"llE3UXWG_euR"},"outputs":[],"source":["model = train_model(X_train, y_train)\n","\n","print(\"Train accuracy = %.4f\" % accuracy_score(y_train, model.predict(X_train)))\n","print(\"Test accuracy = %.4f\" % accuracy_score(y_test, model.predict(X_test)))"]},{"cell_type":"markdown","metadata":{"id":"4vRwbILf_euR"},"source":["### TF-IDF\n","\n","Ещё один способ работы с текстовыми данными — [TF-IDF](https://en.wikipedia.org/wiki/Tf–idf) (**T**erm **F**requency–**I**nverse **D**ocument **F**requency). Рассмотрим коллекцию текстов $D$.  Для каждого уникального слова $t$ из документа $d \\in D$ вычислим следующие величины:\n","\n","1. Term Frequency – количество вхождений слова в отношении к общему числу слов в тексте:\n","$$\\text{tf}(t, d) = \\frac{n_{td}}{\\sum_{t \\in d} n_{td}},$$\n","где $n_{td}$ — количество вхождений слова $t$ в текст $d$.\n","1. Inverse Document Frequency\n","$$\\text{idf}(t, D) = \\log \\frac{\\left| D \\right|}{\\left| \\{d\\in D: t \\in d\\} \\right|},$$\n","где $\\left| \\{d\\in D: t \\in d\\} \\right|$ – количество текстов в коллекции, содержащих слово $t$.\n","\n","Тогда для каждой пары (слово, текст) $(t, d)$ вычислим величину:\n","\n","$$\\text{tf-idf}(t,d, D) = \\text{tf}(t, d)\\cdot \\text{idf}(t, D).$$\n","\n","Отметим, что значение $\\text{tf}(t, d)$ корректируется для часто встречающихся общеупотребимых слов при помощи значения $\\text{idf}(t, D)$."]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2019-09-28T19:10:20.548074Z","start_time":"2019-09-28T19:10:19.877266Z"},"id":"OkwiTKz7_euR"},"outputs":[],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","vectorizer = TfidfVectorizer(encoding='utf8')\n","_ = vectorizer.fit(texts_train)\n","len(vectorizer.vocabulary_)"]},{"cell_type":"markdown","metadata":{"id":"drvaRi-O_euR"},"source":["На выходе получаем разреженную матрицу."]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2019-09-28T19:10:20.569113Z","start_time":"2019-09-28T19:10:20.550328Z"},"id":"m3cQaCos_euR"},"outputs":[],"source":["vectorizer.transform(texts_train[:1])"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2019-09-28T19:10:20.601343Z","start_time":"2019-09-28T19:10:20.574776Z"},"id":"bKLJAynF_euR"},"outputs":[],"source":["print(vectorizer.transform(texts[:1]).indices)\n","print(vectorizer.transform(texts[:1]).data)"]},{"cell_type":"markdown","metadata":{"id":"ggtMchJO_euR"},"source":["Подберем оптимальные гиперпараметры по сетке и обучим модель."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8GR7qyFr_euR"},"outputs":[],"source":["X_train = vectorizer.transform(texts_train)\n","X_test = vectorizer.transform(texts_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z2eWl1gs_euR"},"outputs":[],"source":["model = train_model(X_train, y_train)\n","\n","print(\"Train accuracy = %.4f\" % accuracy_score(y_train, model.predict(X_train)))\n","print(\"Test accuracy = %.4f\" % accuracy_score(y_test, model.predict(X_test)))"]},{"cell_type":"markdown","metadata":{"id":"OMy_Ux5P_euR"},"source":["## Стемминг и лемматизация\n","\n","Заметим, что одно и то же слово может встречаться в различных формах (например, \"сотрудник\" и \"сотрудника\"), но описанные выше методы интерпретируют их как различные слова, что делает признаковое описание избыточным. Устранить эту проблему можно при помощи **лемматизации** и **стемминга**.\n","\n","### Стемминг\n","\n","[**Stemming**](https://en.wikipedia.org/wiki/Stemming) –  это процесс нахождения основы слова. В результате применения данной процедуры однокоренные слова, как правило, преобразуются к одинаковому виду.\n","\n","**Примеры стемминга:**\n","\n","| Word        | Stem           |\n","| ----------- |:-------------:|\n","| вагон | вагон |\n","| вагона | вагон |\n","| вагоне | вагон |\n","| вагонов | вагон |\n","| вагоном | вагон |\n","| вагоны | вагон |\n","| важная | важн |\n","| важнее | важн |\n","| важнейшие | важн |\n","| важнейшими | важн |\n","| важничал | важнича |\n","| важно | важн |\n","\n","[Snowball](http://snowball.tartarus.org/) – фрэймворк для написания алгоритмов стемминга. Алгоритмы стемминга отличаются для разных языков и используют знания о конкретном языке – списки окончаний для разных чистей речи, разных склонений и т.д. Пример алгоритма для русского языка – [Russian stemming](http://snowballstem.org/algorithms/russian/stemmer.html)."]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2019-09-28T19:10:21.109437Z","start_time":"2019-09-28T19:10:20.604439Z"},"id":"caxk7GdO_euR"},"outputs":[],"source":["import nltk\n","stemmer = nltk.stem.snowball.RussianStemmer()"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2019-09-28T19:10:21.130340Z","start_time":"2019-09-28T19:10:21.112772Z"},"id":"H9tEPuJd_euR"},"outputs":[],"source":["print(stemmer.stem(u'машинное'), stemmer.stem(u'обучение'))"]},{"cell_type":"markdown","metadata":{"id":"xon0z_-q_euR"},"source":["Попробуем применить **стемминг** для предобработки текста перед векторизацией. Векторизовывать будем с помощью **tf-idf**, так как такой метод показал лучшее качество."]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2019-09-28T19:10:25.274844Z","start_time":"2019-09-28T19:10:21.135760Z"},"id":"MXUfjdsF_euR"},"outputs":[],"source":["stemmer = nltk.stem.snowball.EnglishStemmer()\n","\n","def stem_text(text, stemmer):\n","    tokens = text.split()\n","    return ' '.join(map(lambda w: stemmer.stem(w), tokens))\n","\n","stemmed_texts_train = []\n","for t in tqdm(texts_train):\n","    stemmed_texts_train.append(stem_text(t, stemmer))\n","\n","stemmed_texts_test = []\n","for t in tqdm(texts_test):\n","    stemmed_texts_test.append(stem_text(t, stemmer))"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2019-09-28T19:10:25.290119Z","start_time":"2019-09-28T19:10:25.279611Z"},"id":"I-fy7L3b_euR"},"outputs":[],"source":["print(texts_train[1])"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2019-09-28T19:10:25.306834Z","start_time":"2019-09-28T19:10:25.294638Z"},"id":"d_uaSm8H_euS"},"outputs":[],"source":["print(stemmed_texts_train[1])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lw6u7ktL_euS"},"outputs":[],"source":["vectorizer = TfidfVectorizer(encoding='utf8')\n","_ = vectorizer.fit(stemmed_texts_train)\n","len(vectorizer.vocabulary_)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UPzF_Lh9_euS"},"outputs":[],"source":["X_train = vectorizer.transform(stemmed_texts_train)\n","X_test = vectorizer.transform(stemmed_texts_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tNCW0SWJ_euS"},"outputs":[],"source":["model = train_model(X_train, y_train)\n","\n","print(\"Train accuracy = %.4f\" % accuracy_score(y_train, model.predict(X_train)))\n","print(\"Test accuracy = %.4f\" % accuracy_score(y_test, model.predict(X_test)))"]},{"cell_type":"markdown","metadata":{"id":"0PbqTVbT_euS"},"source":["### Лемматизация\n","\n","[Лемматизация](https://en.wikipedia.org/wiki/Lemmatisation) — процесс приведения слова к его нормальной форме (**лемме**):\n","- для существительных — именительный падеж, единственное число;\n","- для прилагательных — именительный падеж, единственное число, мужской род;\n","- для глаголов, причастий, деепричастий — глагол в инфинитиве."]},{"cell_type":"markdown","metadata":{"id":"XUfMSidG_euV"},"source":["Например, для русского языка есть библиотека pymorphy3."]},{"cell_type":"code","source":["!pip install pymorphy3 -q"],"metadata":{"id":"mPJcuruBLYFi"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2019-09-28T19:10:25.442948Z","start_time":"2019-09-28T19:10:25.311429Z"},"id":"wkAWY_BK_euV"},"outputs":[],"source":["import pymorphy3\n","morph = pymorphy3.MorphAnalyzer()"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2019-09-28T19:10:25.468072Z","start_time":"2019-09-28T19:10:25.447023Z"},"id":"R2QFYO49_euV"},"outputs":[],"source":["morph.parse('играющих')[0]"]},{"cell_type":"markdown","metadata":{"id":"fbMXr0_x_euV"},"source":["Сравним работу стеммера и лемматизатора на примере:"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2019-09-28T19:10:25.489016Z","start_time":"2019-09-28T19:10:25.474203Z"},"id":"COYHP_Yx_euV"},"outputs":[],"source":["stemmer = nltk.stem.snowball.RussianStemmer()\n","print(stemmer.stem('играющих'))"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2019-09-28T19:10:25.502206Z","start_time":"2019-09-28T19:10:25.493051Z"},"id":"KT45Ci08_euV"},"outputs":[],"source":["print(morph.parse('играющих')[0].normal_form)"]},{"cell_type":"code","source":["!pip install pymystem3 -q"],"metadata":{"id":"mnEx65MRMTA1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from pymystem3 import Mystem\n","\n","mystem_analyzer = Mystem()\n","\n","homonym1 = \"За время обучения я прослушал больше сорока курсов.\"\n","homonym2 = \"Сорока своровала блестящее украшение со стола.\"\n","\n","# корректно определил части речи\n","# NUM — числительное\n","# S — существительное\n","print(mystem_analyzer.analyze(homonym1)[-5])\n","print(mystem_analyzer.analyze(homonym2)[0])"],"metadata":{"id":"ZVQ77BjFMJDc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lQpNhar6_euV"},"source":["Для английского языка будем пользоваться лемматизатором из библиотеки **nltk**."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z_Lm5MrC_euV"},"outputs":[],"source":["from nltk.stem import WordNetLemmatizer\n","\n","nltk.download('wordnet')\n","\n","lemmatizer = WordNetLemmatizer()\n","\n","def lemmatize_text(text, stemmer):\n","    tokens = text.split()\n","    return ' '.join(map(lambda w: lemmatizer.lemmatize(w), tokens))\n","\n","lemmatized_texts_train = []\n","for t in tqdm(texts_train):\n","    lemmatized_texts_train.append(lemmatize_text(t, stemmer))\n","\n","lemmatized_texts_test = []\n","for t in tqdm(texts_test):\n","    lemmatized_texts_test.append(lemmatize_text(t, stemmer))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4rdwQeFr_euV"},"outputs":[],"source":["print(lemmatized_texts_train[1])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7GfA8cFt_euV"},"outputs":[],"source":["print(stemmed_texts_train[1])"]},{"cell_type":"markdown","metadata":{"id":"HDRiyXMT_euV"},"source":["Лемматизируем наш корпус применим tf-idf векторизацию и обучим модель."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6Yz7OAJE_euV"},"outputs":[],"source":["vectorizer = TfidfVectorizer(encoding='utf8')\n","_ = vectorizer.fit(lemmatized_texts_train)\n","len(vectorizer.vocabulary_)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YoES-DPn_euV"},"outputs":[],"source":["X_train = vectorizer.transform(lemmatized_texts_train)\n","X_test = vectorizer.transform(lemmatized_texts_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jPea6JLR_euV"},"outputs":[],"source":["model = train_model(X_train, y_train)\n","\n","print(\"Train accuracy = %.4f\" % accuracy_score(y_train, model.predict(X_train)))\n","print(\"Test accuracy = %.4f\" % accuracy_score(y_test, model.predict(X_test)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7EsJnLNK_euV"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"Au1_38rx_euV"},"source":["## Трансформация признаков и целевой переменной"]},{"cell_type":"markdown","metadata":{"id":"XG4t38N6_euV"},"source":["Разберёмся, как может влиять трансформация признаков или целевой переменной на качество модели."]},{"cell_type":"markdown","metadata":{"id":"0kAfYa8f_euV"},"source":["### Логарифмирование\n","\n","Воспользуется датасетом с ценами на дома, с которым мы уже сталкивались ранее ([House Prices: Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview))."]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2019-09-28T19:10:27.600964Z","start_time":"2019-09-28T19:10:25.507394Z"},"id":"Dt42n35p_euV"},"outputs":[],"source":["!wget  -O 'train.csv' -q 'https://www.dropbox.com/s/syfy4lb6xb7wdlx/_train_sem4.csv?dl=0'"]},{"cell_type":"code","source":["# Либо так:\n","\n","#!wget https://raw.githubusercontent.com/SergeyKorpachev/math-faculty-ml/refs/heads/main/2026/seminars/seminar07/train.csv"],"metadata":{"id":"Cd0KuDb-P32B"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2019-09-28T19:10:27.669451Z","start_time":"2019-09-28T19:10:27.604504Z"},"id":"mzmdXGPx_euV"},"outputs":[],"source":["data = pd.read_csv('train.csv')\n","\n","data = data.drop(columns=[\"Id\"])\n","\n","# Удалите \"SalePrice\" и сформируйте выборки X, y.\n","y = ... # Ваш код сюда :)\n","X = ... # Ваш код сюда :)"]},{"cell_type":"markdown","metadata":{"id":"UjI5llnR_euV"},"source":["Оставим только числовые признаки, пропуски заменим средним значением."]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2019-09-28T19:10:28.464075Z","start_time":"2019-09-28T19:10:28.401693Z"},"id":"4J1VCpde_euW"},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.3, random_state=100)\n","\n","numeric_data = X_train.select_dtypes([np.number])\n","numeric_data_mean = numeric_data.mean()\n","numeric_features = numeric_data.columns\n","\n","# Как правильно нужно заполнить пропуски?\n","X_train = ... # Ваш код сюда :)\n","X_test = ... # Ваш код сюда :)"]},{"cell_type":"markdown","metadata":{"id":"489MpIDK_euW"},"source":["Посмотрим на распределение целевой переменной"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2019-09-28T19:10:28.386207Z","start_time":"2019-09-28T19:10:27.671842Z"},"id":"28pa6MHY_euW"},"outputs":[],"source":["plt.figure(figsize=(12, 5))\n","\n","plt.subplot(1, 2, 1)\n","sns.distplot(y, label='target')\n","plt.title('target')\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"ExecuteTime":{"end_time":"2019-09-26T13:06:11.582867Z","start_time":"2019-09-26T13:06:11.570901Z"},"id":"oq0lTIwu_euW"},"source":["Видим, что распределения несимметричные с тяжёлыми правыми хвостами.\n","\n","Если разбирать линейную регрессию c MSE ошибкой с [вероятностной](https://github.com/esokolov/ml-course-hse/blob/master/2018-fall/seminars/sem04-linregr.pdf) точки зрения, то можно получить, что шум должен быть распределён нормально. Поэтому лучше, когда целевая переменная распределена также нормально.\n","\n","Если прологарифмировать целевую переменную, то её распределение станет больше похоже на нормальное:"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2019-09-28T19:10:28.771830Z","start_time":"2019-09-28T19:10:28.469382Z"},"id":"i7L4M-43_euW"},"outputs":[],"source":["sns.distplot(np.log(y+1), label='target')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"vpDG18D4_euW"},"source":["Сравним качество линейной регрессии в двух случаях:\n","1. Целевая переменная без изменений.\n","2. Целевая переменная прологарифмирована.\n","\n","Не забудем вернуть во втором случае взять экспоненту от предсказаний!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6cVfH9zd_euW"},"outputs":[],"source":["def train_model(X_train, y_train):\n","    alphas = np.logspace(-2, 3, 10)\n","    searcher = GridSearchCV(Ridge(), [{'alpha': alphas}],\n","                            scoring='neg_root_mean_squared_error', cv=10, n_jobs=-1)\n","    searcher.fit(X_train, np.log(y_train+1))\n","\n","    best_alpha = searcher.best_params_[\"alpha\"]\n","    print(\"Best alpha = %.4f\" % best_alpha)\n","\n","    return searcher.best_estimator_.fit(X_train, y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2019-09-28T19:10:28.833250Z","start_time":"2019-09-28T19:10:28.801394Z"},"id":"LYqtBJXu_euW"},"outputs":[],"source":["model = train_model(X_train, y_train)\n","\n","y_pred_train = model.predict(X_train)\n","y_pred_test = model.predict(X_test)\n","\n","# Как посчитать RMSE?\n","... # Ваш код сюда :)\n","... # Ваш код сюда :)"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2019-09-28T19:10:28.860025Z","start_time":"2019-09-28T19:10:28.840089Z"},"id":"fsfN9FAG_euW"},"outputs":[],"source":["model = train_model(X_train, np.log(y_train+1))\n","\n","y_pred_train = np.exp(model.predict(X_train)) - 1\n","y_pred_test = np.exp(model.predict(X_test)) - 1\n","\n","# Как посчитать RMSE?\n","... # Ваш код сюда :)\n","... # Ваш код сюда :)"]},{"cell_type":"markdown","metadata":{"id":"v7zb5ZtP_euW"},"source":["Попробуем аналогично логарифмировать один из признаков, имеющих также смещённое распределение (этот признак был вторым по важности!)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FqdHqi8m_euW"},"outputs":[],"source":["plt.figure(figsize=(12, 5))\n","\n","plt.subplot(1, 2, 1)\n","sns.distplot(y, label='GrLivArea')\n","plt.title('GrLivArea')\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2019-09-28T19:10:28.883024Z","start_time":"2019-09-28T19:10:28.866508Z"},"id":"68OpLuZU_euW"},"outputs":[],"source":["X_train.GrLivArea = np.log(X_train.GrLivArea + 1)\n","X_test.GrLivArea = np.log(X_test.GrLivArea + 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2019-09-28T19:10:28.915423Z","start_time":"2019-09-28T19:10:28.887909Z"},"id":"064sZyBY_euW"},"outputs":[],"source":["model = train_model(X_train, y_train)\n","\n","y_pred_train = model.predict(X_train)\n","y_pred_test = model.predict(X_test)\n","\n","print(\"Train RMSE = %.4f\" % mean_squared_error(y_train, y_pred_train) ** 0.5)\n","print(\"Test RMSE = %.4f\" % mean_squared_error(y_test, y_pred_test) ** 0.5)"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2019-09-28T19:10:28.946812Z","start_time":"2019-09-28T19:10:28.919832Z"},"id":"2EjlRm73_euW"},"outputs":[],"source":["model = train_model(X_train, np.log(y_train+1))\n","\n","y_pred_train = np.exp(model.predict(X_train)) - 1\n","y_pred_test = np.exp(model.predict(X_test)) - 1\n","\n","print(\"Train RMSE = %.4f\" % mean_squared_error(y_train, y_pred_train) ** 0.5)\n","print(\"Test RMSE = %.4f\" % mean_squared_error(y_test, y_pred_test) ** 0.5)"]},{"cell_type":"markdown","metadata":{"id":"4MRbD5Cr_euW"},"source":["~Как видим, логарифмирование признака уменьшило ошибку на тренировочной выборке, но на тестовой выборке ошибка увеличилась.~"]},{"cell_type":"markdown","metadata":{"id":"D6Ayyexz_euW"},"source":["## Категориальные признаки"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v5gkGAJe_euW"},"outputs":[],"source":["! pip install category_encoders -q"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-dlLFuIA_euW"},"outputs":[],"source":["from category_encoders.target_encoder import TargetEncoder\n","\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uiOkh8JC_euW"},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.3, random_state=100)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PdfFD5rU_euW"},"outputs":[],"source":["numeric = list(X_train.select_dtypes(include=np.number).columns)\n","categorical = list(X_train.dtypes[X_train.dtypes == \"object\"].index)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tSV2lYYi_euW"},"outputs":[],"source":["X_train[categorical] = X_train[categorical].fillna(\"NotGiven\")\n","X_test[categorical] = X_test[categorical].fillna(\"NotGiven\")\n","\n","numeric_data_mean = X_train[numeric_features].mean()\n","X_train[numeric] = X_train[numeric].fillna(numeric_data_mean)\n","X_test[numeric] = X_test[numeric].fillna(numeric_data_mean)"]},{"cell_type":"markdown","metadata":{"id":"hQ7RKkj3_euW"},"source":["### One Hot Encoder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AIogJOyQ_euX"},"outputs":[],"source":["column_transformer = ColumnTransformer([\n","    ('ohe', OneHotEncoder(handle_unknown=\"ignore\"), categorical)\n","], remainder='passthrough')\n","\n","pipeline = Pipeline(steps=[\n","    ('ohe', column_transformer),\n","    ('regression', Ridge())\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CmOz360-_euX"},"outputs":[],"source":["alphas = np.logspace(-2, 5, 10)\n","searcher = GridSearchCV(pipeline, [{'regression__alpha': alphas}],\n","                        scoring='neg_root_mean_squared_error', cv=10, n_jobs=-1)\n","searcher.fit(X_train, np.log(y_train+1))\n","\n","best_alpha = searcher.best_params_[\"regression__alpha\"]\n","print(\"Best alpha = %.4f\" % best_alpha)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n_9iMIPB_euX"},"outputs":[],"source":["model = searcher.best_estimator_\n","\n","y_pred_train = np.exp(model.predict(X_train)) - 1\n","y_pred_test = np.exp(model.predict(X_test)) - 1\n","\n","print(\"Train RMSE = %.4f\" % mean_squared_error(y_train, y_pred_train) ** 0.5)\n","print(\"Test RMSE = %.4f\" % mean_squared_error(y_test, y_pred_test) ** 0.5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rlfwO8hq_euX"},"outputs":[],"source":["print('Features before OHE:', len(numeric) + len(categorical))\n","print('Features after OHE:', len(model['regression'].coef_))"]},{"cell_type":"markdown","metadata":{"id":"tXb893Hm_euX"},"source":["Видим, что OHE кодирование признаков привело к колоссальному переобучению, попробуем что-нибудь умнее."]},{"cell_type":"markdown","metadata":{"id":"O-GAqGou_euX"},"source":["### Счетчики (mean target encoding)"]},{"cell_type":"markdown","metadata":{"id":"bSz5SAhc_euX"},"source":["При кодировании категориального признака каждое его значение будем заменять на среднее значение целевой переменной для всех объектов с такой категорией.\n","\n","$$\n","g_j(x, X) = \\frac{\\sum_{i=1}^{\\ell}\\left[f_j(x)=f_j\\left(x_i\\right)\\right] y_i}{\\sum_{i=1}^{\\ell}\\left[f_j(x)=f_j\\left(x_i\\right)\\right]}\n","$$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7-ZVpan9_euX"},"outputs":[],"source":["column_transformer = ColumnTransformer([\n","    ('te', TargetEncoder(smoothing=1.0), categorical)\n","], remainder='passthrough')\n","\n","pipeline = Pipeline(steps=[\n","    ('scale', column_transformer),\n","    ('regression', Ridge())\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rRwZ6Wb0_euX"},"outputs":[],"source":["alphas = np.logspace(-2, 3, 10)\n","searcher = GridSearchCV(pipeline, [{'regression__alpha': alphas}],\n","                        scoring='neg_root_mean_squared_error', cv=10, n_jobs=-1)\n","searcher.fit(X_train, np.log(y_train+1))\n","\n","best_alpha = searcher.best_params_[\"regression__alpha\"]\n","print(\"Best alpha = %.4f\" % best_alpha)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oTR6YhLS_euX"},"outputs":[],"source":["model = searcher.best_estimator_\n","\n","y_pred_train = np.exp(model.predict(X_train)) - 1\n","y_pred_test = np.exp(model.predict(X_test)) - 1\n","\n","print(\"Train RMSE = %.4f\" % mean_squared_error(y_train, y_pred_train) ** 0.5)\n","print(\"Test RMSE = %.4f\" % mean_squared_error(y_test, y_pred_test) ** 0.5)"]},{"cell_type":"markdown","metadata":{"id":"aR1BNKsN_euX"},"source":["Гораздо лучше!"]},{"cell_type":"markdown","metadata":{"id":"xx7eq4g2_euX"},"source":["## Транзакционные данные"]},{"cell_type":"markdown","metadata":{"id":"AA7MdHlS_euX"},"source":["Напоследок посмотрим, как можно извлекать признаки из транзакционных данных.\n","\n","Транзакционные данные характеризуются тем, что есть много строк, характеризующихся моментов времени и некоторым числом (суммой денег, например). При этом если это банк, то каждому человеку принадлежит не одна транзакция, а чаще всего надо предсказывать некоторые сущности для клиентов. Таким образом, надо получить признаки для пользователей из множества их транзакций. Этим мы и займёмся.\n","\n","Для примера возьмём данные [отсюда](https://www.kaggle.com/regivm/retailtransactiondata/). Задача детектирования фродовых клиентов."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wWgLzB19_euX"},"outputs":[],"source":["!wget  -O 'Retail_Data_Response.csv' -q 'https://www.dropbox.com/s/le9icl9syo22thh/Retail_Data_Response.csv?dl=0'\n","!wget  -O 'Retail_Data_Transactions.csv' -q 'https://www.dropbox.com/s/obsxryxpfsdz3ut/Retail_Data_Transactions.csv?dl=0'"]},{"cell_type":"code","source":["# Либо так:\n","\n","#!wget https://raw.githubusercontent.com/SergeyKorpachev/math-faculty-ml/refs/heads/main/2026/seminars/seminar07/Retail_Data_Response.csv\n","#!wget https://raw.githubusercontent.com/SergeyKorpachev/math-faculty-ml/refs/heads/main/2026/seminars/seminar07/Retail_Data_Transactions.csv"],"metadata":{"id":"K24dwG0PTNrK"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2019-09-28T19:10:34.314276Z","start_time":"2019-09-28T19:10:34.231504Z"},"id":"vucz64Dl_euX"},"outputs":[],"source":["customers = pd.read_csv('Retail_Data_Response.csv')\n","transactions = pd.read_csv('Retail_Data_Transactions.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2019-09-28T19:10:34.339478Z","start_time":"2019-09-28T19:10:34.316390Z"},"id":"5Nm9ApEq_euX"},"outputs":[],"source":["customers.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2019-09-28T19:10:34.393258Z","start_time":"2019-09-28T19:10:34.345560Z"},"id":"JsQyuiof_euX"},"outputs":[],"source":["transactions.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2019-09-28T19:10:35.873166Z","start_time":"2019-09-28T19:10:34.443594Z"},"id":"FkkWL1MX_euX"},"outputs":[],"source":["import datetime\n","\n","transactions.trans_date = transactions.trans_date.apply(\n","    lambda x: datetime.datetime.strptime(x, '%d-%b-%y'))"]},{"cell_type":"markdown","metadata":{"id":"maHPSB24_euX"},"source":["Посмотрим на распределение целевой переменной:"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2019-09-28T19:10:35.891923Z","start_time":"2019-09-28T19:10:35.875688Z"},"id":"MUbkMDus_euX"},"outputs":[],"source":["customers.response.mean()"]},{"cell_type":"markdown","metadata":{"id":"YAZTdgIk_euX"},"source":["Получаем примерно 1 к 9 положительных примеров. Если такие данные разбивать на части для кросс валидации, то может получиться так, что в одну из частей попадёт слишком мало положительных примеров, а в другую — наоборот. На случай такого неравномерного баланса классов есть StratifiedKFold, который бьёт данные так, чтобы баланс классов во всех частях был одинаковым."]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2019-09-28T19:10:35.909700Z","start_time":"2019-09-28T19:10:35.898951Z"},"id":"TakQpYf4_euX"},"outputs":[],"source":["from sklearn.model_selection import StratifiedKFold"]},{"cell_type":"markdown","metadata":{"id":"JR6AlTeD_euX"},"source":["Когда строк на каждый объект много, можно считать различные статистики. Например, средние, минимальные и максимальные суммы, потраченные клиентом, количество транзакий, ..."]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2019-09-28T19:10:36.003307Z","start_time":"2019-09-28T19:10:35.914780Z"},"id":"8bP8m69a_euY"},"outputs":[],"source":["agg_transactions = transactions.groupby('customer_id').tran_amount.agg(\n","    ['mean', 'std', 'count', 'min', 'max']).reset_index()\n","\n","data = pd.merge(customers, agg_transactions, how='left', on='customer_id')\n","\n","data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2019-09-28T19:10:36.210653Z","start_time":"2019-09-28T19:10:36.008305Z"},"id":"VdX7yIgt_euY"},"outputs":[],"source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import cross_val_score\n","\n","np.mean(cross_val_score(\n","    LogisticRegression(solver='newton-cg'),\n","    X=data.drop(['customer_id', 'response'], axis=1),\n","    y=data.response,\n","    cv=StratifiedKFold(n_splits=3),\n","    scoring='roc_auc'))"]},{"cell_type":"markdown","metadata":{"id":"xb0Z3iqn_euY"},"source":["Но каждая транзакция снабжена датой! Можно посчитать статистики только по свежим транзакциям. Добавим их."]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2019-09-28T19:10:36.235061Z","start_time":"2019-09-28T19:10:36.218040Z"},"id":"1UERZXqB_euY"},"outputs":[],"source":["transactions.trans_date.min(), transactions.trans_date.max()"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2019-09-28T19:10:36.858305Z","start_time":"2019-09-28T19:10:36.240648Z"},"id":"k2oktGN5_euY"},"outputs":[],"source":["agg_transactions = transactions.loc[transactions.trans_date.apply(\n","    lambda x: x.year == 2014)].groupby('customer_id').tran_amount.agg(\n","    ['mean', 'std', 'count', 'min', 'max']).reset_index()"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2019-09-28T19:10:36.886707Z","start_time":"2019-09-28T19:10:36.860734Z"},"id":"rtrOQZZB_euY"},"outputs":[],"source":["data = pd.merge(data, agg_transactions, how='left', on='customer_id', suffixes=('', '_2014'))\n","data = data.fillna(0)"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2019-09-28T19:10:37.020392Z","start_time":"2019-09-28T19:10:36.889377Z"},"scrolled":true,"id":"ZQSPNcjF_euY"},"outputs":[],"source":["np.mean(cross_val_score(\n","    LogisticRegression(solver='newton-cg'),\n","    X=data.drop(['customer_id', 'response'], axis=1),\n","    y=data.response,\n","    cv=StratifiedKFold(n_splits=3),\n","    scoring='roc_auc'))"]},{"cell_type":"markdown","metadata":{"id":"oL-GHSru_euY"},"source":["Можно также считать дату первой и последней транзакциями пользователей, среднее время между транзакциями и прочее."]},{"cell_type":"code","source":[],"metadata":{"id":"WfKlFWNkUJph"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":true},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"position":{"height":"144px","left":"792px","right":"20px","top":"166px","width":"350px"},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}